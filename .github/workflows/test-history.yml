# Test History and Metrics Tracking
# Collects and stores test results, coverage, and performance metrics over time

name: üìä Test History Tracker

on:
  # Run after main CI/CD pipeline completes
  workflow_run:
    workflows: ["üöÄ CI/CD Pipeline"]
    types: [completed]
  
  # Allow manual execution for data collection
  workflow_dispatch:

permissions:
  contents: read
  pages: write
  id-token: write

jobs:
  collect-metrics:
    name: üìä Collect Test Metrics
    runs-on: ubuntu-latest
    if: github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch'
    
    steps:
      - name: üì• Checkout Repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          ref: ${{ github.event.workflow_run.head_branch || github.ref }}
      
      - name: üêç Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: üì¶ Install Dependencies
        run: |
          pip install requests beautifulsoup4 matplotlib seaborn pandas
      
      - name: üì• Download Test Artifacts
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            
            // Get workflow run ID
            const runId = context.payload.workflow_run?.id || '${{ github.run_id }}';
            
            // Download artifacts from the CI run
            const artifacts = await github.rest.actions.listWorkflowRunArtifacts({
              owner: context.repo.owner,
              repo: context.repo.repo,
              run_id: runId,
            });
            
            // Create metrics directory
            if (!fs.existsSync('test-metrics')) {
              fs.mkdirSync('test-metrics', { recursive: true });
            }
            
            // Download each test result artifact
            for (const artifact of artifacts.data.artifacts) {
              if (artifact.name.includes('test-results')) {
                console.log(`Downloading artifact: ${artifact.name}`);
                
                const download = await github.rest.actions.downloadArtifact({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  artifact_id: artifact.id,
                  archive_format: 'zip',
                });
                
                fs.writeFileSync(`test-metrics/${artifact.name}.zip`, Buffer.from(download.data));
              }
            }
      
      - name: üìä Process Test Results
        run: |
          python3 << 'EOF'
          import json
          import xml.etree.ElementTree as ET
          import os
          import zipfile
          from datetime import datetime
          import glob
          
          def extract_test_metrics():
              """Extract test metrics from downloaded artifacts"""
              metrics = {
                  'timestamp': datetime.now().isoformat(),
                  'commit': '${{ github.event.workflow_run.head_sha || github.sha }}',
                  'branch': '${{ github.event.workflow_run.head_branch || github.ref_name }}',
                  'platforms': {}
              }
              
              # Extract zip files
              for zip_file in glob.glob('test-metrics/*.zip'):
                  with zipfile.ZipFile(zip_file, 'r') as zip_ref:
                      zip_ref.extractall('test-metrics/')
              
              # Process unit test results
              for xml_file in glob.glob('test-metrics/**/unit-test-results.xml', recursive=True):
                  platform = os.path.basename(os.path.dirname(xml_file))
                  
                  try:
                      tree = ET.parse(xml_file)
                      root = tree.getroot()
                      
                      # Extract test statistics
                      tests = int(root.get('tests', 0))
                      failures = int(root.get('failures', 0))
                      errors = int(root.get('errors', 0))
                      time = float(root.get('time', 0))
                      
                      metrics['platforms'][platform] = {
                          'unit_tests': {
                              'total': tests,
                              'passed': tests - failures - errors,
                              'failed': failures,
                              'errors': errors,
                              'duration': time,
                              'success_rate': (tests - failures - errors) / tests * 100 if tests > 0 else 0
                          }
                      }
                  except Exception as e:
                      print(f"Error processing {xml_file}: {e}")
              
              # Process system test results
              for xml_file in glob.glob('test-metrics/**/system-test-results.xml', recursive=True):
                  platform = os.path.basename(os.path.dirname(xml_file))
                  
                  try:
                      tree = ET.parse(xml_file)
                      root = tree.getroot()
                      
                      tests = int(root.get('tests', 0))
                      failures = int(root.get('failures', 0))
                      time = float(root.get('time', 0))
                      
                      if platform not in metrics['platforms']:
                          metrics['platforms'][platform] = {}
                      
                      metrics['platforms'][platform]['system_tests'] = {
                          'total': tests,
                          'passed': tests - failures,
                          'failed': failures,
                          'duration': time,
                          'success_rate': (tests - failures) / tests * 100 if tests > 0 else 0
                      }
                  except Exception as e:
                      print(f"Error processing {xml_file}: {e}")
              
              return metrics
          
          # Extract and save metrics
          metrics = extract_test_metrics()
          
          # Ensure test-history directory exists
          os.makedirs('test-history', exist_ok=True)
          
          # Save current metrics
          with open('test-history/latest.json', 'w') as f:
              json.dump(metrics, f, indent=2)
          
          # Append to historical data
          history_file = 'test-history/history.jsonl'
          with open(history_file, 'a') as f:
              f.write(json.dumps(metrics) + '\n')
          
          print("Test metrics processed and saved")
          print(json.dumps(metrics, indent=2))
          EOF
      
      - name: üìà Generate Test History Dashboard
        run: |
          python3 << 'EOF'
          import json
          import matplotlib.pyplot as plt
          import matplotlib.dates as mdates
          from datetime import datetime
          import pandas as pd
          import os
          
          def generate_dashboard():
              """Generate test history dashboard"""
              
              # Read historical data
              history = []
              if os.path.exists('test-history/history.jsonl'):
                  with open('test-history/history.jsonl', 'r') as f:
                      for line in f:
                          if line.strip():
                              history.append(json.loads(line))
              
              if not history:
                  print("No historical data available")
                  return
              
              # Create dashboard HTML
              html_content = """
              <!DOCTYPE html>
              <html>
              <head>
                  <title>Dashcam Project - Test History Dashboard</title>
                  <style>
                      body { font-family: Arial, sans-serif; margin: 20px; background: #f5f5f5; }
                      .container { max-width: 1200px; margin: 0 auto; background: white; padding: 20px; border-radius: 8px; }
                      .header { text-align: center; color: #2563eb; border-bottom: 2px solid #e5e7eb; padding-bottom: 20px; }
                      .metrics-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin: 20px 0; }
                      .metric-card { background: #f8fafc; border: 1px solid #e5e7eb; border-radius: 8px; padding: 15px; }
                      .metric-title { font-weight: bold; color: #374151; margin-bottom: 10px; }
                      .metric-value { font-size: 2em; font-weight: bold; }
                      .success { color: #22c55e; }
                      .warning { color: #f59e0b; }
                      .error { color: #ef4444; }
                      .chart-container { margin: 20px 0; text-align: center; }
                      table { width: 100%; border-collapse: collapse; margin: 20px 0; }
                      th, td { border: 1px solid #e5e7eb; padding: 8px; text-align: left; }
                      th { background: #f3f4f6; }
                  </style>
              </head>
              <body>
                  <div class="container">
                      <div class="header">
                          <h1>üß™ Dashcam Project Test History</h1>
                          <p>Automated test results and metrics tracking</p>
                          <p><strong>Last Updated:</strong> {timestamp}</p>
                      </div>
              """.format(timestamp=datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC'))
              
              # Get latest metrics
              latest = history[-1] if history else {}
              
              # Add current metrics summary
              html_content += """
                      <div class="metrics-grid">
              """
              
              for platform, data in latest.get('platforms', {}).items():
                  unit_tests = data.get('unit_tests', {})
                  system_tests = data.get('system_tests', {})
                  
                  html_content += f"""
                          <div class="metric-card">
                              <div class="metric-title">üì± {platform}</div>
                              <div class="metric-value success">{unit_tests.get('success_rate', 0):.1f}%</div>
                              <div>Unit Test Success Rate</div>
                              <div style="margin-top: 10px;">
                                  <small>
                                      Unit: {unit_tests.get('passed', 0)}/{unit_tests.get('total', 0)} passed<br>
                                      System: {system_tests.get('passed', 0)}/{system_tests.get('total', 0)} passed
                                  </small>
                              </div>
                          </div>
                  """
              
              html_content += """
                      </div>
                      
                      <h2>üìà Test History Trends</h2>
                      <div class="chart-container">
                          <p><em>Charts would be generated here with matplotlib/plotly in a full implementation</em></p>
                      </div>
                      
                      <h2>üìã Recent Test Runs</h2>
                      <table>
                          <tr>
                              <th>Date</th>
                              <th>Commit</th>
                              <th>Branch</th>
                              <th>Platform</th>
                              <th>Unit Tests</th>
                              <th>System Tests</th>
                              <th>Success Rate</th>
                          </tr>
              """
              
              # Add recent test run data
              for entry in history[-10:]:  # Last 10 runs
                  timestamp = entry.get('timestamp', '')
                  commit = entry.get('commit', '')[:8]
                  branch = entry.get('branch', '')
                  
                  for platform, data in entry.get('platforms', {}).items():
                      unit_tests = data.get('unit_tests', {})
                      system_tests = data.get('system_tests', {})
                      
                      unit_rate = unit_tests.get('success_rate', 0)
                      system_rate = system_tests.get('success_rate', 0)
                      overall_rate = (unit_rate + system_rate) / 2 if unit_rate and system_rate else max(unit_rate, system_rate)
                      
                      status_class = 'success' if overall_rate >= 95 else 'warning' if overall_rate >= 80 else 'error'
                      
                      html_content += f"""
                              <tr>
                                  <td>{timestamp[:19]}</td>
                                  <td><code>{commit}</code></td>
                                  <td>{branch}</td>
                                  <td>{platform}</td>
                                  <td>{unit_tests.get('passed', 0)}/{unit_tests.get('total', 0)}</td>
                                  <td>{system_tests.get('passed', 0)}/{system_tests.get('total', 0)}</td>
                                  <td class="{status_class}">{overall_rate:.1f}%</td>
                              </tr>
                      """
              
              html_content += """
                      </table>
                      
                      <div style="margin-top: 40px; text-align: center; color: #6b7280;">
                          <p>Generated by GitHub Actions ‚Ä¢ Dashcam Project CI/CD</p>
                      </div>
                  </div>
              </body>
              </html>
              """
              
              # Save dashboard
              os.makedirs('test-dashboard', exist_ok=True)
              with open('test-dashboard/index.html', 'w') as f:
                  f.write(html_content)
              
              print("Test dashboard generated")
          
          generate_dashboard()
          EOF
      
      - name: üì§ Upload Test History
        uses: actions/upload-artifact@v3
        with:
          name: test-history-dashboard
          path: |
            test-history/
            test-dashboard/
          retention-days: 90
      
      - name: üìä Commit Test History
        if: github.event_name == 'workflow_run' && github.event.workflow_run.head_branch == 'main'
        run: |
          # Configure git
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          # Create test-history branch if it doesn't exist
          git fetch origin test-history:test-history 2>/dev/null || git checkout -b test-history
          git checkout test-history
          
          # Copy test history files
          mkdir -p test-history test-dashboard
          cp -r test-history/* test-history/ 2>/dev/null || true
          cp -r test-dashboard/* test-dashboard/ 2>/dev/null || true
          
          # Commit if there are changes
          git add test-history/ test-dashboard/
          if ! git diff --staged --quiet; then
            git commit -m "Update test history and dashboard - $(date)"
            git push origin test-history
          fi

  deploy-dashboard:
    name: üöÄ Deploy Test Dashboard
    runs-on: ubuntu-latest
    needs: collect-metrics
    if: github.event.workflow_run.head_branch == 'main' || github.ref == 'refs/heads/main'
    
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    
    steps:
      - name: üì• Checkout test-history branch
        uses: actions/checkout@v4
        with:
          ref: test-history
      
      - name: üîß Setup Pages
        uses: actions/configure-pages@v3
      
      - name: üì§ Upload Pages Artifact
        uses: actions/upload-pages-artifact@v2
        with:
          path: 'test-dashboard'
      
      - name: üöÄ Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v2
